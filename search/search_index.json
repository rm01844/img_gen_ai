{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 AI Image Generation API Documentation","text":"<p>Welcome to the AI Image Generation &amp; Editing API, powered by Google Vertex AI ( &amp; Gemini Models) and deployed securely on Railway.</p> <p>This service allows developers to: - \ud83c\udfa8 Generate images from text prompts - \ud83e\ude84 Edit existing images using AI guidance - \ud83d\udd10 Access both web UI and REST API endpoints with token-based authentication</p>"},{"location":"#documentation-overview","title":"\ud83d\udcd8 Documentation Overview","text":"Section Description \ud83e\udde9 API Reference Details of each endpoint (<code>/generate</code>, <code>/edit</code>), request/response formats, and sample payloads \ud83e\uddf0 Application Overview Explains how the Flask app, Vertex AI integration, and authentication system work \ud83d\ude80 Deployment Guide Step-by-step guide for deploying to Railway or another cloud platform \ud83d\udcc4 OpenAPI Spec Swagger schema to auto-import into Postman / Insomnia / client SDKs"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#base-url","title":"Base URL","text":"<p>https://web-production-fc79.up.railway.app/</p> <p>All endpoints are relative to this base URL.</p>"},{"location":"#endpoints","title":"\ud83d\udd27 Endpoints","text":"Endpoint Method Description <code>/generate</code> POST Generate image(s) from text prompt <code>/edit</code> POST Edit uploaded image based on prompt <code>/login</code> GET/POST Superadmin login (OTP required) <code>/logout</code> GET End session"},{"location":"#available-models","title":"\ud83e\udde0 Available Models","text":"Function Model ID Version Provider Text \u2192 Image <code>imagen-4.0-generate-001</code> v4.0 Google Vertex AI Image \u2192 Image (Edit) <code>gemini-2.5-flash-image</code> v3.0 Google Vertex AI Text \u2192 Text (Prompt) <code>gemini 2.5-pro</code> v2.5 Google Vertex AI <p>\ud83e\udded Navigation For API details \u2192 start with API Reference</p> <p>For deployment or local setup \u2192 see Deployment Guide</p> <p>For technical app overview \u2192 see Application Structure</p> <p>For client-side import \u2192 use OpenAPI Spec</p>"},{"location":"deployment/","title":"Deployment Guide","text":""},{"location":"deployment/#docsdeploymentmd","title":"\ud83e\udde9 <code>docs/deployment.md</code>","text":""},{"location":"deployment/#deployment-guide","title":"\ud83d\ude80 Deployment Guide","text":"<p>This document explains how to deploy and configure the AI Image Generator API using Google Vertex AI and Flask.</p>"},{"location":"deployment/#environment-variables","title":"\ud83e\uddf0 Environment Variables","text":"<p>Create a <code>.env</code> file (or configure on Railway):</p> Variable Description Example <code>PROJECT_ID</code> Your Google Cloud project ID <code>your_project_ID</code> <code>LOCATION</code> Vertex AI region <code>us-central1</code> <code>SERVICE_KEY_JSON</code> JSON contents of the service account key <code>service account key (.json)</code> <code>API_TOKEN</code> Secret used for client API authentication <code>API_TOKEN</code> <code>SUPERADMIN_OTP</code> OTP for admin web login <code>custom_otp</code> <code>SECRET_KEY</code> Flask session encryption key <code>random_flask_secret</code> <code>PORT</code> App port <code>8080</code> <p>\u26a0\ufe0f Never commit <code>.env</code> or <code>.json</code> files to GitHub.</p>"},{"location":"deployment/#directory-structure","title":"\ud83e\uddf1 Directory Structure","text":"<pre><code>Img_gen_AI/\n\u251c\u2500\u2500 img_gen_ai/\n\u2502   \u251c\u2500\u2500 app.py\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 templates/\n\u2502       \u251c\u2500\u2500 index.html\n\u2502       \u2514\u2500\u2500 login.html\n\u2514\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md \n\u2502   \u251c\u2500\u2500 reference/\n\u2502       \u2514\u2500\u2500 api.md\n\u2502       \u2514\u2500\u2500 app.md\n\u2502   \u2514\u2500\u2500 deployment.md\n\u251c\u2500\u2500 Procfile\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"deployment/#deployment-railway","title":"\u2699\ufe0f Deployment (Railway)","text":"<ol> <li>Push to GitHub</li> <li> <p>Ensure your <code>requirements.txt</code> and <code>Procfile</code> exist.</p> </li> <li> <p>Create Railway project</p> </li> <li>Connect GitHub repo.</li> <li> <p>Set environment variables in Settings \u2192 Variables.</p> </li> <li> <p>Build &amp; deploy    Railway auto-detects Flask from <code>Procfile</code>:</p> </li> </ol> <p>web: gunicorn img_gen_ai.app:app</p> <ol> <li>Access the app</li> </ol> <p>https://example.railway.app/login</p>"},{"location":"deployment/#security-checklist","title":"\ud83d\udd10 Security Checklist","text":"<ul> <li>[ ] Store tokens and service keys only in environment variables  </li> <li>[ ] Use long, random API tokens (\u2265 32 chars)  </li> <li>[ ] Rotate API tokens periodically  </li> <li>[ ] Restrict GCP service account to minimal permissions  </li> <li>[ ] Enable HTTPS (Railway provides automatically)</li> </ul>"},{"location":"deployment/#under-the-hood","title":"\ud83e\udde0 Under the Hood","text":"Component Purpose Flask Web framework and routing Vertex AI - Nano Banana AI image generation and editing Google Service Account Authenticates your app with Vertex Gunicorn Production web server MkDocs Documentation generator"},{"location":"deployment/#verification","title":"\u2705 Verification","text":"<p>After deployment, test your API using: <pre><code>curl -X POST https://example.railway.app/api \\\n-H \"Authorization: Bearer &lt;YOUR_API_TOKEN&gt;\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"prompt\":\"a mountain landscape in watercolor\",\"number_of_images\":1}'\n</code></pre> Expected Response:</p> <p>{   \"image_urls\": [\"/static/generated_.png\"] }"},{"location":"deployment/#credits","title":"\ud83e\uddfe Credits","text":"<p>Google Cloud Vertex AI Nano Banana</p> <p>Flask for backend</p> <p>Railway.app for deployment</p> <p>MkDocs for documentation</p>"},{"location":"reference/api/","title":"\ud83e\udde0 AI Image Generation API","text":"<p>This API enables developers to generate and edit images using Google Vertex AI\u2019s Imagen models through a secure Flask REST interface.</p>"},{"location":"reference/api/#base-url","title":"\ud83d\udd17 Base URL","text":"<p>The application is deployed and hosted on Railway.app. This URL serves as the root endpoint for all API requests.</p> <p>Base URL: https://web-production-fc79.up.railway.app/</p> <p>Example: To access <code>/generate</code>, send a POST request to <code>https://web-production-fc79.up.railway.app/generate</code></p>"},{"location":"reference/api/#authentication","title":"\ud83d\udd10 Authentication","text":"<p>All API endpoints require a valid Bearer token to be included in the request header. This token is used to authenticate and authorize client requests.</p> <p>Header Format:</p> <p>Authorization: Bearer"},{"location":"reference/api/#obtaining-the-token","title":"\ud83d\udd11 Obtaining the Token","text":"<p>Bearer tokens are provided by the API administrator upon onboarding. Each client or integration partner will receive a unique token associated with their account.</p> <p>If you have not received your token, please contact the API administrator or project owner.</p> <p>\u26a0\ufe0f Important: - Do not share your token publicly or embed it in client-side code. - Tokens are tied to your project identity and usage limits. - Rotate tokens periodically or immediately if compromised.</p>"},{"location":"reference/api/#example-authenticated-request","title":"\ud83e\udde9 Example: Authenticated Request","text":"<pre><code>curl -X POST https://web-production-fc79.up.railway.app/generate \\\n  -H \"Authorization: Bearer your_api_token_here\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\":\"a serene landscape with mountains and clouds\"}'\n</code></pre>"},{"location":"reference/api/#requirements","title":"\ud83d\udce6 Requirements","text":"<ul> <li>Python 3.10+</li> <li>Google Cloud Vertex AI API enabled and copy the API_TOKEN</li> <li>A service account key (.json) with the <code>Vertex AI User</code> and <code>Storage Admin</code> roles</li> <li>Railway.app or Render account for deployment</li> <li>MkDocs (optional) for documentation hosting</li> </ul>"},{"location":"reference/api/#endpoints","title":"\u2728 Endpoints","text":""},{"location":"reference/api/#post-generate","title":"POST /generate","text":"<p>Generate one or more images from a text prompt.</p> <p>Request Body</p> Field Type Required Description <code>prompt</code> string \u2705 Description of the image to generate <code>number_of_images</code> int \u274c Default: <code>1</code>, number of images to create <code>aspect_ratio</code> string \u274c Default: <code>\"1:1\"</code>, aspect ratio such as <code>16:9</code>, <code>3:4</code>, <code>9:16</code> <code>negative_prompt</code> string \u274c Optional text describing what to avoid <p>Example</p> <p>{   prompt: \"A futuristic city skyline at sunset\",   number_of_images: 2,   aspect_ratio: \"16:9\" }</p> <p>Response</p> <p>{   image_urls: [     \"/static/generated_8fa3b3.png\",     \"/static/generated_19df1d.png\"   ] }</p> <p>Underlying Model: imagen-4.0-generate-001 Provider: Google Vertex AI</p>"},{"location":"reference/api/#post-edit","title":"POST /edit","text":"<p>This endpoint enables AI-powered image editing by combining Google Gemini 2.5 Pro (for intelligent prompt refinement and feedback) with Gemini 2.5 Flash Image (for photo-realistic image generation).</p>"},{"location":"reference/api/#workflow-overview","title":"\ud83e\udde9 Workflow Overview","text":"<ol> <li>Prompt Refinement (Gemini 2.5 Pro)</li> <li>The user\u2019s raw text prompt is rewritten by Gemini to be concise, spatially descriptive, and better aligned with Gemini's generation semantics.</li> <li> <p>Example transformation:      &gt; \"Turn this person into an action figure in orange packaging\"      \u2192 \"Create a high-resolution product photo of the same person as a realistic action figure packaged in an orange blister pack with accessories arranged symmetrically and a name label at the top.\"</p> </li> <li> <p>Image Generation (Gemini 2.5 Flash Image)</p> </li> <li> <p>The refined prompt and uploaded reference image are sent to Gemini 2.5 Flash Image to produce the edited output.</p> </li> <li> <p>Automated Feedback (Gemini Vision Review)</p> </li> <li>Gemini optionally evaluates the generated image for identity and layout consistency,      then suggests a refined prompt for re-generation to improve results.</li> </ol>"},{"location":"reference/api/#authentication_1","title":"\ud83d\udd10 Authentication","text":"<p>All requests must include a valid Bearer Token in the HTTP header:</p> <pre><code>Authorization: Bearer &lt;YOUR_API_TOKEN&gt; \n</code></pre> <p>Request (multipart/form-data)</p> Field Type Required Description <code>image</code> file \u2705 The source image to edit <code>prompt</code> string \u2705 Natural-language description of desired changes <code>number_of_images</code> int \u274c Default: <code>1</code>, number of variations to return <code>negative_prompt</code> string \u274c Optional text describing elements to avoid. <code>edit_strength</code> float \u274c Degree of transformation, range 0.1\u20131.0 (default 0.55) <code>enhance_detail</code> bool \u274c Enhance visual detail and sharpness (true by default) <p>Example </p> <pre><code>curl -X POST https://web-production-fc79.up.railway.app/edit \\\n  -H \"Authorization: Bearer &lt;YOUR_API_TOKEN&gt;\" \\\n  -F \"image=@ben_philip.jpg\" \\\n  -F \"prompt=Create a photo-realistic action figure of the same person in orange packaging with accessories neatly arranged.\" \\\n  -F \"number_of_images=2\"\n</code></pre> <p>Response</p> <p>{   image_urls: [     \"/static/edited_a13c0d.png\",     \"/static/edited_b28d9f.png\"   ] }</p> <p>Underlying Model: Gemini 2.5 Flash Image + Gemini 2.5 Pro Provider: Google Vertex AI</p>"},{"location":"reference/api/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n  A[Client Request (POST /edit)] --&gt; B[Flask API (Python)]\n  B --&gt;|Multipart form: prompt + image| C[Gemini 2.5 Pro (Prompt Refinement)]\n  C --&gt;|Refined prompt| D[Nano Banana - Gemini 2.5 Flash Image(Image Editing)]\n  D --&gt;|Generated image URLs| E[Gemini Vision Review (Optional Feedback)]\n  E --&gt;|Revised prompt (if needed)| D\n  D --&gt;|Final image URLs| F[Flask API Response]\n  F --&gt;|JSON Response| G[Client (Web UI / API Consumer)]\n</code></pre>"},{"location":"reference/api/#error-handling","title":"\u26a0\ufe0f Error Handling","text":"HTTP Code Meaning Example Cause 400 Bad Request Missing prompt or file 401 Unauthorized Invalid or missing token 404 Not Found Endpoint mismatch 500 Internal Error Vertex API or server issue <p>\ud83e\udde0 Notes</p> <ul> <li> <p>Images are saved temporarily under /static/.</p> </li> <li> <p>Watermarks may be applied by Vertex AI for compliance.</p> </li> <li> <p>The API auto-refreshes credentials from your GCP service account.</p> </li> </ul>"},{"location":"reference/app/","title":"\ud83e\udde9 AI Image Generator &amp; Editing \u2014 API Reference","text":"<p>This section documents all Flask routes and functions exposed by the backend.</p>"},{"location":"reference/app/#img_gen_ai.app--ai-image-generator-api","title":"AI Image Generator API","text":"<p>A Flask application that provides endpoints to generate and edit images using Google Vertex AI's Imagen models.</p> <p>This module includes authentication, text-to-image generation, and image-editing endpoints.</p>"},{"location":"reference/app/#img_gen_ai.app.get_face_app","title":"get_face_app","text":"<pre><code>get_face_app()\n</code></pre> <p>Lazy initialization of face detection</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def get_face_app():\n    \"\"\"Lazy initialization of face detection\"\"\"\n    global _face_app\n    if _face_app is None:\n        _face_app = FaceAnalysis(name=\"buffalo_l\", providers=[\n                                 \"CPUExecutionProvider\"])\n        _face_app.prepare(ctx_id=0)\n    return _face_app\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.extract_skin_tone_description","title":"extract_skin_tone_description","text":"<pre><code>extract_skin_tone_description(img_path)\n</code></pre> <p>Extract detailed skin tone information from the reference image. Returns a description that can be used in prompts to preserve ethnicity.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def extract_skin_tone_description(img_path: str) -&gt; str:\n    \"\"\"\n    Extract detailed skin tone information from the reference image.\n    Returns a description that can be used in prompts to preserve ethnicity.\n    \"\"\"\n    try:\n        face_app = get_face_app()\n        img = cv2.imread(img_path)\n        if img is None:\n            return \"\"\n\n        faces = face_app.get(img)\n        if not faces:\n            return \"\"\n\n        # Get the largest face\n        face = max(faces, key=lambda f: (\n            f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))\n        x1, y1, x2, y2 = face.bbox.astype(int)\n\n        # Extract face region\n        face_region = img[y1:y2, x1:x2]\n\n        # Calculate average color in LAB space (more perceptually uniform)\n        face_lab = cv2.cvtColor(face_region, cv2.COLOR_BGR2LAB)\n        avg_color = np.mean(face_lab, axis=(0, 1))\n\n        L, a, b = avg_color\n\n        # Classify skin tone based on L channel and a/b values\n        if L &gt; 200:\n            tone = \"very fair skin tone with pale complexion\"\n        elif L &gt; 170:\n            tone = \"fair skin tone with light complexion\"\n        elif L &gt; 140:\n            tone = \"medium-light skin tone with warm undertones\"\n        elif L &gt; 110:\n            tone = \"medium skin tone with neutral undertones\"\n        elif L &gt; 80:\n            tone = \"medium-dark skin tone with olive undertones\"\n        elif L &gt; 50:\n            tone = \"dark skin tone with rich brown complexion\"\n        else:\n            tone = \"very dark skin tone with deep brown complexion\"\n\n        # Add undertone information based on a and b channels\n        if a &lt; -5:\n            tone += \", greenish undertones\"\n        elif b &gt; 15:\n            tone += \", warm golden undertones\"\n        elif b &lt; 0:\n            tone += \", cool blue undertones\"\n\n        return tone\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Skin tone extraction failed: {e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.build_improved_edit_prompt","title":"build_improved_edit_prompt","text":"<pre><code>build_improved_edit_prompt(user_prompt, skin_tone_desc='')\n</code></pre> <p>Build a structured prompt that enforces constraints while allowing creative changes.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def build_improved_edit_prompt(user_prompt: str, skin_tone_desc: str = \"\") -&gt; str:\n    \"\"\"\n    Build a structured prompt that enforces constraints while allowing creative changes.\n    \"\"\"\n    # Parse the user prompt to identify key elements\n    lines = [ln.strip() for ln in user_prompt.split(\"\\n\") if ln.strip()]\n\n    # Build constraint section\n    constraint_text = f\"\"\"STRICT IDENTITY CONSTRAINTS:\n- Maintain EXACT facial features, proportions, and bone structure from reference\n- Preserve EXACT skin tone: {skin_tone_desc or \"match the reference image precisely\"}\n- Keep original ethnicity and racial characteristics unchanged\n- Do NOT alter, lighten, or darken skin color under any circumstances\n\nCOMPOSITION RULES:\n- Follow the layout description precisely\n- Only include objects explicitly mentioned in the prompt\n- Do NOT add weapons, guns, sticks, or any items not specified\n- Arrange items exactly as described in numbered lists\n- Use clean, minimal composition without extra clutter\n\nEDIT INSTRUCTIONS:\n{chr(10).join([f\"\u2022 {ln}\" for ln in lines])}\n\nTECHNICAL QUALITY:\n- Studio lighting with soft shadows\n- Professional product photography style\n- Sharp focus on main subject\n- Natural color rendering (avoid oversaturation)\n- Clean white or neutral background unless specified otherwise\"\"\"\n\n    return constraint_text\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.refine_prompt_with_gemini","title":"refine_prompt_with_gemini","text":"<pre><code>refine_prompt_with_gemini(raw_prompt, skin_tone='')\n</code></pre> <p>Use Gemini to restructure prompt with strict constraints.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def refine_prompt_with_gemini(raw_prompt: str, skin_tone: str = \"\") -&gt; str:\n    \"\"\"Use Gemini to restructure prompt with strict constraints.\"\"\"\n    try:\n        if not raw_prompt.strip():\n            return \"Modify this image while preserving all facial features and skin tone.\"\n\n        system_instruction = f\"\"\"You are a prompt engineer for Google Vertex Imagen 3 image editing API.\nRewrite this prompt to be:\n1. EXTREMELY EXPLICIT about preserving the person's exact facial features and skin tone\n2. HIGHLY STRUCTURED with clear numbered steps for layout\n3. SPECIFIC about what NOT to include (e.g., no weapons, no extra objects)\n\nCRITICAL RULES:\n- Always start with: \"IDENTITY PRESERVATION: Keep the exact face, {skin_tone or 'skin tone'}, and proportions from the reference.\"\n- Break complex layouts into numbered steps\n- List items to EXCLUDE in a \"DO NOT ADD\" section\n- Use professional photography terminology\n- Keep it under 500 words\n\nOriginal prompt: {raw_prompt}\"\"\"\n\n        response = gemini.generate_content(system_instruction)\n        refined = getattr(response, \"text\", \"\") or \"\"\n\n        if not refined.strip() or len(refined.strip()) &lt; 10:\n            print(\"\u26a0\ufe0f Gemini returned empty text, using fallback\")\n            return build_improved_edit_prompt(raw_prompt, skin_tone)\n\n        return refined.strip()\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Gemini refinement failed: {e}\")\n        return build_improved_edit_prompt(raw_prompt, skin_tone)\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.apply_skin_tone_preservation","title":"apply_skin_tone_preservation","text":"<pre><code>apply_skin_tone_preservation(reference_path, generated_path)\n</code></pre> <p>Apply aggressive color correction to match reference skin tone. Runs AFTER generation to fix any tone drift.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def apply_skin_tone_preservation(reference_path: str, generated_path: str) -&gt; None:\n    \"\"\"\n    Apply aggressive color correction to match reference skin tone.\n    Runs AFTER generation to fix any tone drift.\n    \"\"\"\n    try:\n        face_app = get_face_app()\n\n        ref_img = cv2.imread(reference_path)\n        gen_img = cv2.imread(generated_path)\n\n        if ref_img is None or gen_img is None:\n            return\n\n        ref_faces = face_app.get(ref_img)\n        gen_faces = face_app.get(gen_img)\n\n        if not ref_faces or not gen_faces:\n            print(\"\u26a0\ufe0f No faces detected for skin preservation\")\n            return\n\n        ref_face = max(ref_faces, key=lambda f: (\n            f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))\n        gen_face = max(gen_faces, key=lambda f: (\n            f.bbox[2] - f.bbox[0]) * (f.bbox[3] - f.bbox[1]))\n\n        rx1, ry1, rx2, ry2 = ref_face.bbox.astype(int)\n        gx1, gy1, gx2, gy2 = gen_face.bbox.astype(int)\n\n        # bounds clamp\n        Href, Wref = ref_img.shape[:2]\n        Hgen, Wgen = gen_img.shape[:2]\n        rx1, ry1 = max(0, rx1), max(0, ry1)\n        rx2, ry2 = min(Wref, rx2), min(Href, ry2)\n        gx1, gy1 = max(0, gx1), max(0, gy1)\n        gx2, gy2 = min(Wgen, gx2), min(Hgen, gy2)\n\n        if rx2 &lt;= rx1 or ry2 &lt;= ry1 or gx2 &lt;= gx1 or gy2 &lt;= gy1:\n            return\n\n        ref_face_region = ref_img[ry1:ry2, rx1:rx2]\n        gen_face_region = gen_img[gy1:gy2, gx1:gx2]\n        if ref_face_region.size == 0 or gen_face_region.size == 0:\n            return\n\n        # LAB color space matching\n        ref_lab = cv2.cvtColor(\n            ref_face_region, cv2.COLOR_BGR2LAB).astype(float)\n        gen_lab = cv2.cvtColor(\n            gen_face_region, cv2.COLOR_BGR2LAB).astype(float)\n\n        ref_mean, ref_std = ref_lab.mean(axis=(0, 1)), ref_lab.std(axis=(0, 1))\n        gen_mean, gen_std = gen_lab.mean(axis=(0, 1)), gen_lab.std(axis=(0, 1))\n\n        # Color transfer\n        for i in range(3):\n            gen_lab[:, :, i] = ((gen_lab[:, :, i] - gen_mean[i]) *\n                                (ref_std[i] / (gen_std[i] + 1e-6))) + ref_mean[i]\n\n        gen_lab = np.clip(gen_lab, 0, 255).astype(np.uint8)\n        corrected_face = cv2.cvtColor(gen_lab, cv2.COLOR_LAB2BGR)\n\n        # soft ellipse mask\n        h, w = gen_face_region.shape[:2]\n        mask = np.zeros((h, w), dtype=np.float32)\n        cv2.ellipse(mask, (w//2, h//2), (w//2, h//2), 0, 0, 360, 1, -1)\n        mask = cv2.GaussianBlur(mask, (51, 51), 30)[..., None]\n\n        blended = (corrected_face * mask + gen_face_region *\n                   (1 - mask)).astype(np.uint8)\n        gen_img[gy1:gy2, gx1:gx2] = blended\n        cv2.imwrite(generated_path, gen_img, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n        print(\"\u2705 Skin tone preservation applied\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Skin preservation failed: {e}\")\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.login_required","title":"login_required","text":"<pre><code>login_required(f)\n</code></pre> <p>Decorator that restricts access to logged-in admin users.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>def login_required(f):\n    \"\"\"Decorator that restricts access to logged-in admin users.\"\"\"\n    from functools import wraps\n\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        if not session.get(\"logged_in\"):\n            flash(\"Please log in to access the generator.\", \"warning\")\n            return redirect(url_for(\"login\"))\n        return f(*args, **kwargs)\n    return decorated_function\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.login","title":"login","text":"<pre><code>login()\n</code></pre> <p>Superadmin login page with OTP validation.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Renders the login form or redirects to the index if OTP is valid.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/login\", methods=[\"GET\", \"POST\"])\ndef login() -&gt; Response:\n    \"\"\"Superadmin login page with OTP validation.\n\n    Returns:\n        Response: Renders the login form or redirects to the index if OTP is valid.\n    \"\"\"\n    stored_otp = (os.getenv(\"SUPERADMIN_OTP\") or \"\").strip().replace(\n        \"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\")\n\n    if request.method == \"POST\":\n        entered_otp = (request.form.get(\"otp\") or \"\").strip().replace(\n            \"\\n\", \"\").replace(\"\\r\", \"\").replace(\" \", \"\")\n        print(\n            f\"\ud83d\udd0d DEBUG OTP | Entered='{entered_otp}' | Stored='{stored_otp}' | Match={entered_otp == stored_otp}\")\n\n        if entered_otp == stored_otp:\n            session.permanent = True\n            session[\"is_admin\"] = True\n            print(\"\u2705 OTP accepted \u2014 redirecting to index.\")\n            return redirect(url_for(\"index\"))\n        else:\n            print(\"\u274c Invalid OTP entered.\")\n            return render_template_string(\"\"\"\n                &lt;!DOCTYPE html&gt;\n                &lt;html lang=\"en\"&gt;\n                &lt;head&gt;\n                    &lt;meta charset=\"UTF-8\"&gt;\n                    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n                    &lt;title&gt;Invalid OTP&lt;/title&gt;\n                    &lt;style&gt;\n                        body { font-family: Arial; text-align: center; margin-top: 100px; }\n                        a { color: #007bff; text-decoration: none; }\n                    &lt;/style&gt;\n                &lt;/head&gt;\n                &lt;body&gt;\n                    &lt;h2 style='color:red;'&gt;Invalid OTP&lt;/h2&gt;\n                    &lt;a href='/login'&gt;Try again&lt;/a&gt;\n                &lt;/body&gt;\n                &lt;/html&gt;\n            \"\"\")\n\n    return render_template_string(\"\"\"\n        &lt;!DOCTYPE html&gt;\n        &lt;html lang=\"en\"&gt;\n        &lt;head&gt;\n            &lt;meta charset=\"UTF-8\"&gt;\n            &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n            &lt;title&gt;Superadmin Login&lt;/title&gt;\n            &lt;style&gt;\n                body {\n                    font-family: Arial, sans-serif;\n                    display: flex;\n                    flex-direction: column;\n                    align-items: center;\n                    justify-content: center;\n                    height: 100vh;\n                    background-color: #121212;\n                    color: white;\n                }\n                input, button {\n                    padding: 10px;\n                    margin: 10px;\n                    border: none;\n                    border-radius: 6px;\n                }\n                input {\n                    width: 200px;\n                    text-align: center;\n                }\n                button {\n                    background-color: #4CAF50;\n                    color: white;\n                    cursor: pointer;\n                }\n                button:hover {\n                    background-color: #45a049;\n                }\n            &lt;/style&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;form method=\"POST\"&gt;\n                &lt;h2&gt;Enter Superadmin OTP&lt;/h2&gt;\n                &lt;input type=\"password\" name=\"otp\" placeholder=\"Enter OTP\" required&gt;\n                &lt;button type=\"submit\"&gt;Login&lt;/button&gt;\n            &lt;/form&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n    \"\"\")\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.restrict_access","title":"restrict_access","text":"<pre><code>restrict_access()\n</code></pre> <p>Restrict access to authorized users only.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.before_request\ndef restrict_access() -&gt; Response | None:\n    \"\"\"Restrict access to authorized users only.\"\"\"\n    if request.endpoint not in (\"login\", \"static\") and not session.get(\"is_admin\"):\n        return redirect(url_for(\"login\"))\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.index","title":"index","text":"<pre><code>index()\n</code></pre> <p>Render the main web interface.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/\")\ndef index() -&gt; Response:\n    \"\"\"Render the main web interface.\"\"\"\n    return render_template(\"index.html\")\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.generate_image","title":"generate_image","text":"<pre><code>generate_image()\n</code></pre> <p>Generate image(s) using Vertex AI Imagen model.</p> <p>This endpoint generates one or more images based on the user's prompt and options.</p> <p>Request Body (JSON):     prompt (str): The image description to generate.     number_of_images (int, optional): Number of images to generate (default=1).     aspect_ratio (str, optional): Aspect ratio, e.g. \"1:1\" or \"16:9\".     negative_prompt (str, optional): Objects/concepts to avoid.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>JSON containing a list of image URLs or an error message.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/generate\", methods=[\"POST\"])\ndef generate_image() -&gt; Response:\n    \"\"\"Generate image(s) using Vertex AI Imagen model.\n\n    This endpoint generates one or more images based on the user's prompt and options.\n\n    Request Body (JSON):\n        prompt (str): The image description to generate.\n        number_of_images (int, optional): Number of images to generate (default=1).\n        aspect_ratio (str, optional): Aspect ratio, e.g. \"1:1\" or \"16:9\".\n        negative_prompt (str, optional): Objects/concepts to avoid.\n\n    Returns:\n        Response: JSON containing a list of image URLs or an error message.\n    \"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get(\"prompt\")\n        number_of_images = data.get(\"number_of_images\", \"1\")\n        aspect_ratio = data.get(\"aspect_ratio\", \"1:1\")\n        negative_prompt = data.get(\"negative_prompt\", \"\")\n\n        if not prompt:\n            return jsonify({\"error\": \"No prompt provided\"}), 400\n\n        # Load Imagen model\n        model = ImageGenerationModel.from_pretrained(\"imagen-4.0-generate-001\")\n\n        # Generate image\n        result = model.generate_images(\n            prompt=prompt,\n            number_of_images=int(number_of_images),\n            aspect_ratio=aspect_ratio,\n            negative_prompt=negative_prompt,\n            person_generation=\"allow_all\",\n            safety_filter_level=\"block_few\",\n            add_watermark=False,\n        )\n\n        static_dir = os.path.join(os.path.dirname(__file__), \"static\")\n        os.makedirs(static_dir, exist_ok=True)\n\n        # Save image to static folder\n        image_urls = []\n        for img in result.images:\n            filename = f\"generated_{uuid.uuid4().hex}.png\"\n            output_path = os.path.join(static_dir, filename)\n            img.save(output_path)\n            image_urls.append(f\"/static/{filename}?v={int(time.time())}\")\n\n        print(f\"\u2705 Generated {len(image_urls)} images\")\n        return jsonify({\"image_urls\": image_urls})\n\n    except Exception as e:\n        print(f\"\u274c Generation error: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.edit_image","title":"edit_image","text":"<pre><code>edit_image()\n</code></pre> <p>Edit an uploaded image using Gemini 2.5 Flash Image model.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/edit\", methods=[\"POST\"])\ndef edit_image() -&gt; Response:\n    \"\"\"Edit an uploaded image using Gemini 2.5 Flash Image model.\n    \"\"\"\n    try:\n        raw_prompt = request.form.get(\"prompt\", \"\").strip()\n        if \"image\" not in request.files:\n            return jsonify({\"error\": \"No image uploaded\"}), 400\n\n        uploaded = request.files[\"image\"]\n        temp_path = os.path.join(\n            tempfile.gettempdir(), f\"upload_{uuid.uuid4().hex}.png\")\n        uploaded.save(temp_path)\n\n        with open(temp_path, \"rb\") as f:\n            image_bytes = f.read()\n\n        print(f\"\ud83d\udd8c\ufe0f Editing with Gemini 2.5 Flash Image...\")\n\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash-image\",\n            contents=[\n                {\"role\": \"user\", \"parts\": [\n                    {\"text\": raw_prompt},\n                    {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": image_bytes}}\n                ]}\n            ],\n            config=GenerateContentConfig(\n                response_modalities=[\"IMAGE\"],\n                candidate_count=1,\n            ),\n        )\n\n        static_dir = os.path.join(os.path.dirname(__file__), \"static\")\n        os.makedirs(static_dir, exist_ok=True)\n        filename = f\"edited_{uuid.uuid4().hex}.png\"\n        output_path = os.path.join(static_dir, filename)\n\n        for part in response.candidates[0].content.parts:\n            if hasattr(part, \"inline_data\"):\n                with open(output_path, \"wb\") as f:\n                    f.write(part.inline_data.data)\n\n        print(f\"\u2705 Edit successful: {output_path}\")\n        return jsonify({\"image_urls\": [f\"/static/{filename}?v={int(time.time())}\"]})\n\n    except Exception as e:\n        print(f\"\u274c Edit error: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.chat_edit","title":"chat_edit","text":"<pre><code>chat_edit()\n</code></pre> <p>Iterative editing via chat interface using Gemini 2.5 Flash Image.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/chat_edit\", methods=[\"POST\"])\ndef chat_edit() -&gt; Response:\n    \"\"\"Iterative editing via chat interface using Gemini 2.5 Flash Image.\"\"\"\n    try:\n        data = request.get_json()\n        instruction = data.get(\"instruction\", \"\")\n        last_image = urlparse(data.get(\"image_path\", \"\")).path.lstrip(\"/\")\n\n        if not instruction or not last_image:\n            return jsonify({\"error\": \"Missing instruction or image\"}), 400\n\n        abs_path = os.path.join(os.getcwd(), last_image)\n        if not os.path.exists(abs_path):\n            return jsonify({\"error\": f\"Image not found: {abs_path}\"}), 404\n\n        # \ud83d\udd39 Analyze tone for identity consistency\n        skin_tone = extract_skin_tone_description(abs_path)\n\n        # \ud83d\udd39 Build the full directive prompt\n        directive_prompt = f\"\"\"IDENTITY PRESERVATION: Maintain exact facial features and {skin_tone}.\nCHANGE REQUEST: {instruction}\nCONSTRAINTS:\n- Do NOT alter skin tone or facial structure\n- Only modify elements explicitly mentioned\n- Preserve realism and consistent lighting\n- Professional studio photo quality\"\"\"\n\n        print(\"\ud83e\udde0 Gemini 2.5 Flash Image chat-edit in progress...\")\n\n        # \u2705 Read image bytes (Gemini expects binary, not base64)\n        with open(abs_path, \"rb\") as f:\n            image_bytes = f.read()\n\n        # \ud83d\udd39 Call Gemini 2.5 Flash Image\n        response = client.models.generate_content(\n            model=\"gemini-2.5-flash-image\",\n            contents=[\n                {\n                    \"role\": \"user\",\n                    \"parts\": [\n                        {\"text\": directive_prompt},\n                        {\"inline_data\": {\"mime_type\": \"image/png\", \"data\": image_bytes}}\n                    ],\n                }\n            ],\n            config=GenerateContentConfig(\n                response_modalities=[\"IMAGE\"],\n                candidate_count=1,\n            ),\n        )\n\n        # \u2705 Save the edited image to /static\n        static_dir = os.path.join(os.path.dirname(__file__), \"static\")\n        os.makedirs(static_dir, exist_ok=True)\n        filename = f\"chat_edit_{uuid.uuid4().hex}.png\"\n        output_path = os.path.join(static_dir, filename)\n\n        for part in response.candidates[0].content.parts:\n            if hasattr(part, \"inline_data\"):\n                with open(output_path, \"wb\") as f:\n                    f.write(part.inline_data.data)\n\n        # Optionally re-apply your tone-preserving logic\n        apply_skin_tone_preservation(abs_path, output_path)\n\n        print(f\"\u2705 Chat-edit successful: {output_path}\")\n        return jsonify({\"image_url\": f\"/static/{filename}?v={int(time.time())}\"})\n\n    except Exception as e:\n        print(f\"\u274c Chat-edit error: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n</code></pre>"},{"location":"reference/app/#img_gen_ai.app.logout","title":"logout","text":"<pre><code>logout()\n</code></pre> <p>Log out the current admin session.</p> Source code in <code>img_gen_ai/app.py</code> <pre><code>@app.route(\"/logout\")\ndef logout() -&gt; Response:\n    \"\"\"Log out the current admin session.\"\"\"\n    session.pop(\"is_admin\", None)\n    print(\"\ud83d\udc4b Superadmin logged out.\")\n    return redirect(url_for(\"login\"))\n</code></pre>"}]}